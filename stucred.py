# -*- coding: utf-8 -*-
"""StuCRED_Data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lNoi-G59_SfL8H2Ki_EcB9X3BsD8mrSU
"""

import numpy as np
import pandas as pd
import random

# ==============================================
# 1. CONFIGURATIONS
# ==============================================

NUM_ROWS = 1000
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
random.seed(RANDOM_SEED)

# Define final value ranges for each feature
FEATURE_RANGES = {
    "CGPA":  (40.0,  100.0),    # range: 40–100
    "U_R":   (0.005, 0.2),      # range: 0.005–0.2
    "C_D":   (0.5,   1.5),      # range: 0.5–1.5
    "I_S":   (0.0,   1.0),      # range: 0–1
    "E_S":   (0.0,   3.0),      # range: 0–3
    "C_C":   (0.0,   1.0),      # range: 0–1
    "E_P":   (0.5,   2.0),      # range: 0.5–2
    "L_D":   (0.0,   4.0),      # range: 0–4
    "P_F":   (0.5,   3.0),      # range: 0.5–3
    "B_S":   (0.0,   1.0),      # range: 0–1
    "S_R":   (0.0,   80.0),     # range: 0–80
    "S_H":   (0.0,   50.0),     # range: 0–50
    "B_P":   (50.0,  100.0),    # range: 50–100
    "M_B":   (50.0,  100.0),    # range: 50–100
    "P_I":   (20.0,  80.0),     # range: 20–80
    "U_T":   (0.5,   2.0),      # range: 0.5–2
    "M_U":   (0.5,   2.0),      # range: 0.5–2
    "S_M":   (0.5,   2.0),      # range: 0.5–2
    "L_Q":   (0.0,   100.0),    # range: 0–100
    "T_E":   (0.0,   70.0),     # range: 0–70
    "A_T":   (50.0,  100.0),    # range: 50–100
}
"""
# CGPA Score (0-100 scale, typical 40-100)
    "U_R": (0.005, 0.2),     # University Ranking measure (1/rank in 1–200 range)
    "C_D": (0.5, 1.5),       # Course Difficulty
    "I_S": (0.0, 1.0),       # Internship Score
    "E_S": (0.0, 3.0),       # Part-time Employment ratio
    "C_C": (0.0, 1.0),       # Certification Courses
    "E_P": (0.5, 2.0),       # Expected Post-Graduation Salary ratio
    "L_D": (0.0, 4.0),       # Existing Loans/Debts
    "P_F": (0.5, 3.0),       # Parental Financial Background
    "B_S": (0.0, 1.0),       # Bank Account Stability
    "S_R": (0.0, 80.0),      # Savings Rate
    "S_H": (0.0, 50.0),      # Spending Habits
    "B_P": (50.0, 100.0),    # Bill Payments
    "M_B": (50.0, 100.0),    # Mobile Bill Payments
    "P_I": (20.0, 80.0),     # Percentage Income on Essentials
    "U_T": (0.5, 2.0),       # UPI Transactions ratio
    "M_U": (0.5, 2.0),       # Mobile Usage ratio
    "S_M": (0.5, 2.0),       # Social Media Activity ratio
    "L_Q": (0.0, 100.0),     # Loan Intent Quiz Score
    "T_E": (0.0, 70.0),      # Time Spent on Educational Apps vs. Entertainment
    "A_T": (50.0, 100.0),    # Attendance
"""

FEATURE_NAMES = list(FEATURE_RANGES.keys())

# ==============================================
# 2. LATENT FACTORS
# ==============================================
# We create 4 latent factors to induce correlations among features.
# Each factor ~ N(0,1).
# Later, each actual feature = some linear combination of these factors + noise,
# then scaled to the final range.

def generate_latent_factors(num_rows):
    """
    Returns arrays of shape (num_rows,) for each latent factor.
    """
    academic_factor = np.random.normal(loc=0.0, scale=1.0, size=num_rows)
    finance_factor = np.random.normal(loc=0.0, scale=1.0, size=num_rows)
    spending_factor = np.random.normal(loc=0.0, scale=1.0, size=num_rows)
    digital_factor = np.random.normal(loc=0.0, scale=1.0, size=num_rows)
    return academic_factor, finance_factor, spending_factor, digital_factor

# ==============================================
# 3. FEATURE GENERATION
# ==============================================

def min_max_scale(values, feature_name):
    """
    Scale 'values' array to the range specified by FEATURE_RANGES[feature_name].
    """
    min_val, max_val = FEATURE_RANGES[feature_name]
    # Scale to [0, 1] first
    v_min, v_max = values.min(), values.max()
    if v_min == v_max:
        # Avoid division by zero: if the feature is constant, set them to mid-range
        return np.array([0.5*(min_val + max_val)] * len(values))
    values_scaled_01 = (values - v_min) / (v_max - v_min)
    # Now scale to [min_val, max_val]
    values_scaled = values_scaled_01 * (max_val - min_val) + min_val
    return values_scaled

def generate_features(num_rows):
    """
    Generate a dictionary of synthetic features, each shaped (num_rows,).
    """
    # 1) get latent factors
    acad, fin, spend, digi = generate_latent_factors(num_rows)

    # We'll define raw (unscaled) values for each feature
    # as a linear combination of relevant factors + some noise.
    # Each "weight" is chosen to reflect a plausible correlation direction.

    raw_data = {}

    # CGPA ~ positively correlated with "academic_factor"
    # U_R ~ negatively correlated with "academic_factor" (because U_R = 1/rank; better rank => higher factor => higher U_R, which is positive, ironically. Let's just do direct correlation for simplicity).
    # C_D ~ slightly correlated with academic (some courses can be tough if academically strong students pick them), but also some random
    # and so on...
    # This is all just for demonstration.

    raw_data["CGPA"] =  ( 1.2*acad ) + np.random.normal(0, 0.5, num_rows)
    raw_data["U_R"]  =  ( 0.8*acad ) + np.random.normal(0, 0.3, num_rows)
    raw_data["C_D"]  =  ( 0.3*acad ) + np.random.normal(0, 0.8, num_rows)

    raw_data["I_S"]  =  ( 0.5*acad ) + np.random.normal(0, 0.5, num_rows)
    raw_data["E_S"]  =  ( 0.2*acad ) + (0.3*fin) + np.random.normal(0, 0.6, num_rows)
    raw_data["C_C"]  =  ( 0.4*acad ) + np.random.normal(0, 0.4, num_rows)
    raw_data["E_P"]  =  ( 0.4*acad ) + (0.4*fin)  + np.random.normal(0, 0.4, num_rows)

    raw_data["L_D"]  =  (-0.3*fin)   + np.random.normal(0, 0.6, num_rows)  # negative => if finance_factor is higher => less debt ratio
    raw_data["P_F"]  =  ( 0.8*fin ) + np.random.normal(0, 1.0, num_rows)
    raw_data["B_S"]  =  ( 0.6*fin ) + np.random.normal(0, 0.4, num_rows)
    raw_data["S_R"]  =  ( 0.5*fin ) + np.random.normal(0, 1.2, num_rows)

    raw_data["S_H"]  =  ( 0.7*spend ) + np.random.normal(0, 1.0, num_rows)
    raw_data["B_P"]  =  (-0.4*spend ) + np.random.normal(0, 1.0, num_rows)  # negative => if spending factor is high => late payments
    raw_data["M_B"]  =  (-0.3*spend ) + np.random.normal(0, 1.0, num_rows)
    raw_data["P_I"]  =  (-0.5*spend ) + np.random.normal(0, 1.0, num_rows)

    raw_data["U_T"]  =  ( 0.3*digi ) + np.random.normal(0, 0.5, num_rows)
    raw_data["M_U"]  =  ( 0.6*digi ) + np.random.normal(0, 0.5, num_rows)
    raw_data["S_M"]  =  ( 0.8*digi ) + np.random.normal(0, 0.7, num_rows)
    raw_data["L_Q"]  =  (-0.3*digi ) + (0.3*acad) + np.random.normal(0, 2.0, num_rows)
    raw_data["T_E"]  =  ( 0.2*digi ) + (0.4*acad) + np.random.normal(0, 1.0, num_rows)
    raw_data["A_T"]  =  ( 0.5*acad ) + np.random.normal(0, 1.0, num_rows)

    # Convert raw to DataFrame
    df_raw = pd.DataFrame(raw_data)

    # Scale each feature to the desired final range
    for col in df_raw.columns:
        df_raw[col] = min_max_scale(df_raw[col].values, col)

    return df_raw

# ==============================================
# 4. GENERATE loan_paid_back LABEL
# ==============================================
# We'll define a heuristic: if a student has strong academic factor, stable finances,
# good bill payments, lower L_D ratio, etc., they have higher chance of paying back.

def generate_loan_label(df_features):
    """
    Create a binary label (0 or 1) for loan repayment based on a heuristic
    combining multiple features.
    """
    # Weighted combination of features (scaled 0–1 internally where possible).
    # We'll transform each to [0,1] or something comparable, then add up.

    # Some features are already in [0,1] form effectively, but many are in different ranges.
    # We'll do a scaled approach for each:

    # For example:
    #   CGPA (40-100) => scale to 0..1
    #   L_D (0-4) => invert scale
    #   B_P (50-100) => scale to 0..1
    #   S_R (0-80) => scale to 0..1
    #   etc.

    # Let's define small helper:
    def scale_feature(value, minval, maxval):
        return (value - minval) / (maxval - minval)

    # We derive a "repayment_score" in [0,1], then threshold at 0.5 to get the final label.
    repayment_scores = (
         0.25 * scale_feature(df_features["CGPA"], 40, 100)
       + 0.20 * (1 - scale_feature(df_features["L_D"], 0, 4))  # invert L_D
       + 0.20 * scale_feature(df_features["B_P"], 50, 100)
       + 0.15 * scale_feature(df_features["S_R"], 0, 80)
       + 0.10 * (1 - scale_feature(df_features["S_H"], 0, 50)) # invert S_H
       + 0.10 * scale_feature(df_features["A_T"], 50, 100)
    )
    # Convert to binary label
    labels = (repayment_scores >= 0.5).astype(int)
    return labels

# ==============================================
# 5. MAIN SCRIPT
# ==============================================

def main():
    # Generate features
    df_features = generate_features(NUM_ROWS)
    # Make an ID column
    df_features.insert(0, "ID", range(1, NUM_ROWS+1))
    # Generate loan_paid_back
    df_features["loan_paid_back"] = generate_loan_label(df_features)

    # Save CSV
    df_features.to_csv("realistic_synthetic_dataset.csv", index=False)
    print("Realistic synthetic data generated and saved to 'realistic_synthetic_dataset.csv'.")

if __name__ == "__main__":
    main()

import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, BatchNormalization, Dropout
from tensorflow.keras.optimizers import Adam

def main():
    # 1. LOAD DATA
    # ================================================
    df = pd.read_csv("realistic_synthetic_dataset.csv")
    print("Data Shape:", df.shape)

    # 2. SEPARATE FEATURES & TARGET
    # ================================================
    # 'loan_paid_back' is our target. 'ID' is just an identifier.
    FEATURE_COLUMNS = [
        "CGPA","U_R","C_D","I_S","E_S","C_C","E_P","L_D","P_F","B_S",
        "S_R","S_H","B_P","M_B","P_I","U_T","M_U","S_M","L_Q","T_E","A_T"
    ]
    TARGET_COLUMN = "loan_paid_back"

    X = df[FEATURE_COLUMNS].values  # shape: (num_samples, 21)
    y = df[TARGET_COLUMN].values    # shape: (num_samples, )

    # 3. TRAIN-TEST SPLIT
    # ================================================
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    print("Train shape:", X_train.shape, "Test shape:", X_test.shape)

    # 4. NORMALIZE FEATURES
    # ================================================
    scaler = MinMaxScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # 5. BUILD A DEEPER NEURAL NETWORK
    # ================================================
    # We'll build a feed-forward network with multiple Dense layers,
    # ReLU activations, BatchNorm, and Dropout.

    model = Sequential()
    n_features = X_train_scaled.shape[1]

    # Input & first hidden layer
    model.add(Dense(units=64, activation='relu', input_shape=(n_features,)))
    model.add(BatchNormalization())  # helps with stable training
    model.add(Dropout(0.2))          # helps reduce overfitting

    # Second hidden layer
    model.add(Dense(units=32, activation='relu'))
    model.add(BatchNormalization())
    model.add(Dropout(0.2))

    # Third hidden layer
    model.add(Dense(units=16, activation='relu'))
    model.add(BatchNormalization())
    model.add(Dropout(0.2))

    # Output layer: 1 neuron for binary classification
    model.add(Dense(units=1, activation='sigmoid'))

    # 6. COMPILE MODEL
    # ================================================
    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss='binary_crossentropy',
        metrics=['accuracy']
    )

    # 7. TRAIN MODEL (MORE EPOCHS)
    # ================================================
    # Let's train for 50 epochs to give the model time to converge.
    history = model.fit(
        X_train_scaled,
        y_train,
        epochs=50,
        batch_size=32,
        validation_split=0.2,  # 20% of training data for validation
        verbose=1
    )

    # 8. EVALUATE ON TEST SET
    # ================================================
    loss, accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)
    print(f"Test Loss: {loss:.4f}")
    print(f"Test Accuracy: {accuracy:.4f}")

    # 9. DEMO PREDICTIONS
    # ================================================
    # Probability that the student will repay the loan
    y_prob = model.predict(X_test_scaled)
    y_pred = (y_prob >= 0.5).astype(int)

    # Show first 10 predictions
    for i in range(100):
        print(
            f"Sample {i} => Predicted Prob = {y_prob[i][0]:.4f}, "
            f"Pred Class = {y_pred[i][0]}, Actual = {y_test[i]}"
        )
if __name__ == "__main__":
    main()

model.save("loan_repayment_model.keras")

import pickle
with open("scaler.pkl", "wb") as f:
    pickle.dump(scaler, f)